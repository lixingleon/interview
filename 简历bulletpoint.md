# 简历：

使用React+SpringBoot+MySQL技术完成项目搭建。 用户上传宠物描述信息和领养天数数据以后，后台训练预测模型。用户再输入想要预测的宠物的信息，后台就会根据这些信息给出预测结果。

功能有上传文件，特征展示，特征选择，模型训练和预测。

通过切片上传和md5标识的方法，优化了上传功能。实现了大文件秒传和断点续传功能，大大提升了用户在上传大文件和在网络不稳定时的使用体验。

PetFinder 

带领两人小组开发一个宠物领养速度预测的机器学习平台，帮助收容所工作人员提高宠物领养的概率。

为提升开发效率，采用前后端分离架构。主要功能有上传数据，特征展示及选择，模型训练，以及预测领养天数。

前端使用React.JS 框架构建单页web应用，并使用Material-UI和Bootstrap客制化原生组件，提高页面美观性。

后端使用SpringBoot开发，采用Controller-Service-Repository-Model设计模式减少代码耦合，并使用JPA与MySQL进行交互，减少了该模块50%的开发时间。

为优化超大文件传输及网络拥塞导致的文件上传失败问题，通过文件切片和md5标识的方法，实现了大文件秒传和断点续传功能，提升了30%的上传速度并大大提升了用户体验。



软件开发实习生

用Java语言开发了用于Android和IOS应用的本地化自动翻译工具，是公司内第一个投入使用的内部工具。

用XML框架dom4j实现了XML文件和CSV文件之间的转换，并用Google Sheet API生成统一的翻译配置页面，极大提高了Android 和IOS 工程师的协同工作效率。

用Google Translate API将英文字符串翻译成14国语言，并自动生成各语言的字符串文件，减少了开发人员50%的开发时间。

XML to CSV: 

XMLToCSV类： ReadDataFromxml()

Utils类：Generatecsvfromxml()

用google sheet打开，googlesheet作为配置页面

GoogleSheet类：getCSVFromGoogleSheet(). googlesheet产出本地csv文件

Translate类：GeneralTranslate()将本地csv文件翻译成14国语言

​						GenerateTranslatedcsv() 并输出到本地

ExportToAndroid类：



算法实习生

协助算法工程师从零到一搭建推荐算法底层框架，完成了第一期算法项目。

通过SQL脚本分析整站用户流量分布以及链路转化，并主动向数仓同学请教，构建了每日自动化报表，大大提升观测数据的效率。

跑通了2个经典召回算法（协同过滤和矩阵分解），并通过调整训练数据集调优，最终算法上线后带来了月度GMV5%的提升。

调研各大电商平台的用户画像指标体系，从零到一搭建了公司第一个用户画像系统。



网易实习：

1. 用注解+Spring AOP的方式模拟了算法服务的返回结果，解决了本地无法连接算法服务所以无法单测的痛点，并优化了公司的单元测试方案。
2. 主导设计并开发了一个权限系统，用来管理公司内部系统的权限。使用RBAC模型，优化了授权管理的灵活性和颗粒度，并在client端和server端分别使用caffeine和redis缓存，大大提高查询效率，也提升了系统稳定性和并发承载量。还使用了模板方法模式和监听者模式等设计模式，提高了代码的可扩展性和可维护性。
3. 调研调度算法，优化将请求发送给算法服务的方式，将平均长度的请求超时率降低了15%。



# 简历深挖

怎么优化单测的？

注解+AOP的动态代理。

@MockData(mockClass = "", mockMethod = "")

当调用算法方法时，先执行切面方法，在切面方法中，根据开关，来决定调用原方法还是mock方法。mock方法就通过注解中的变量，通过反射来获取。



RBAC模型是什么？

Role based access control。在用户和权限之间增加一层角色。



你为什么要使用模板方法模式？是什么，怎么操作的？

当两个操作流程一样，只有部分细节不一样时，可以将流程抽象出一个模板方法，子类将不一样的细节部分分别重写，这样可以少写很多重复代码。对比caffeine和redis缓存的流程，其实差不多，所以可以把共性的操作抽象出来，作为一个父类，然后两个子类继承父类，并重写父类的抽象方法。

你为什么要使用监听者模式，怎么用的？

监听者模式可以用来更新缓存，用该模式可以实现代码的解耦。

传统方式更新缓存是在update数据的方法中，调用更新缓存接口。这样就把更新缓存的逻辑耦合进update方法里了。通过引入监听者模式，更新数据以后就可以发布一个消息，更新缓存的接口可以订阅此消息。

讲一下调度算法：

之前调用算法服务的方式存在问题。每次都是直接将一整篇文章发送给算法端，算法端处理完再发送回来。这样的问题是一个超长文章会一直占用算法服务。导致其他短请求都超时。

想让算法端处理长文章时，当处理到超时，剩下的句子就不处理了，直接返回超时结果。这样短文章就能及时被处理了。这个逻辑加在算法端会增加复杂度。于是我们在服务端将文章分batch传输给算法端。比如一个120句的文章分12个batch，每个batch10个句子，每个batch都带上同样的起始时间。当算法端发现这个batch的起始时间跟当前时间已经过了1s时，就直接返回超时结果。这样比如处理到第5个batch的时候超时了，后面的batch就不会再处理了。长请求被提前结束，短请求就被照顾到了。





讲一下你的自动翻译工具。

这个工具以安卓的XML文件作为基准，将XML文件转化成CSV文件，再用Google Sheet打开，这样就有了一个配置界面。因为安卓和IOS的工程师的字符串是一样的，所以二者就可以在这个配置界面协同办公。比如删增字符串，增加翻译语言等等。配置好以后可以自动导出成安卓端的XML文件和IOS端的String文件。

讲一下大文件传输的优化方案

首先对文件生成一个md5值，查看数据库的文件表中是否有这个值的文件，如果有就不上传了。这就是大文件秒传。如果没有，在前端将文件切片。在传输每个切片前，先发送请求到后端，check数据库中是否已经有该切片了，如果没有则上传。将所有切片上传完毕后，发送一个请求通知后端将切片合并成一个文件。数据库有一张切片表，一张文件表，当切片都上传完毕，合并成功后，文件表新增一条数据，切片表的20条记录会删除。

比如一个200m的文件，切割成20个10m的文件，就要进行最多40次与服务端的通信。

优化：在第一步check文件是否存在时，如果不存在，就返回已有的切片id。前端将文件切割成20份后，去掉已有的id，将后端没有的切片发送过去。这样最多只要20次通信，通信成本降低了一半。



讲一下OAuth2.0 原理：

客户端将用户导到服务提供端的认证界面。

用户通过扫码，授予客户端获取自己部分数据的权限。

服务提供端将access code放在客户端的回调url后面，将用户导回到客户端。客户端通过url获取了access code

客户端通过access code向服务提供端请求token，获取token后，用token就能获取用户授权的数据了。



# 自我介绍

南加大硕士，西浦和利物浦本科

应聘的岗位是software engineer intern。

我跟其他应聘者相比有三大优势

1. 我学习能力突出，本科几乎满绩。拿到了优秀学术奖学金。也就是说我学东西是很快的。
2. 第二我也乐于学。我对实用的技术很感兴趣，这里实用技术分两类，一类能切实提升用户体验。比如之前我个人的项目里用到的OAuth2.0授权，大文件秒传和切片上传，再比如我现在实习中即将做的二级缓存，其中包含缓存的设计，比如调度算法的优化等等，这些都能让用户用得更舒服。
3. 另一类是能提升开发体验的技术。比如我通过AOP来优化单元测试的流程。比如在开发中运用一些设计模式，来解耦，提高代码的可维护性。这样在持续迭代的过程中可以让大家尽量少做重复劳动。把实用技术应用到我的项目中去会让我很有成就感。
4. 第三我实习经历丰富，有三段实习。所以能更快地熟悉上手业务，也能够灵活处理同事间的关系，所以我也希望我能有这个机会，在微软的平台上，能和像您一样优秀的同事，一起去做一些事情。
